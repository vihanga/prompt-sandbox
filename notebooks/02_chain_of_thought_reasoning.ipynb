{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2: Chain-of-Thought Prompting for Reasoning Tasks\n",
    "\n",
    "## The Experiment\n",
    "\n",
    "**Question I wanted to answer**: Does explicitly asking the model to \"show its work\" actually improve accuracy on reasoning tasks?\n",
    "\n",
    "**Why this matters**: Reading papers about CoT is one thing, but I wanted to see the difference myself with controlled experiments.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Testing on math word problems because:\n",
    "- Clear right/wrong answers (no subjective evaluation)\n",
    "- Common failure mode for LLMs (jumping to wrong conclusion)\n",
    "- Easy to measure improvement quantitatively\n",
    "\n",
    "**Two approaches**:\n",
    "1. **Direct**: Just ask for the answer\n",
    "2. **Chain-of-Thought**: Ask to break down reasoning step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Requirements\n",
    "\n",
    "**To run this notebook:**\n",
    "\n",
    "```bash\n",
    "# 1. Install the package\n",
    "cd /path/to/prompt-sandbox\n",
    "pip install -e .\n",
    "\n",
    "# 2. Install notebook dependencies\n",
    "pip install jupyter matplotlib\n",
    "\n",
    "# 3. Run this notebook\n",
    "jupyter notebook notebooks/\n",
    "```\n",
    "\n",
    "**What this notebook does:**\n",
    "- Uses GPT-2 (small model, ~500MB download)\n",
    "- Takes 5-10 minutes to run on CPU\n",
    "- No GPU required\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from prompt_sandbox.config.schema import PromptConfig\n",
    "from prompt_sandbox.prompts.template import PromptTemplate\n",
    "from prompt_sandbox.models.huggingface import HuggingFaceBackend\n",
    "from prompt_sandbox.experiments import AsyncExperimentRunner, ExperimentConfig\n",
    "from prompt_sandbox.evaluators import BLEUEvaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(\"\u2705 Ready to experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Problems\n",
    "\n",
    "Mix of arithmetic, algebra, and word problems with varying difficulty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math word problems - from simple to tricky\n",
    "test_problems = [\n",
    "    {\n",
    "        \"problem\": \"Sarah has 23 apples. She gives 8 to her friend. How many apples does she have left?\",\n",
    "        \"answer\": \"15\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A train travels 60 miles in 45 minutes. What is its average speed in miles per hour?\",\n",
    "        \"answer\": \"80\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If 5 shirts cost $125, how much would 8 shirts cost at the same price per shirt?\",\n",
    "        \"answer\": \"200\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A rectangle has a length that is 3 times its width. If the perimeter is 48 cm, what is the width?\",\n",
    "        \"answer\": \"6\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"John is twice as old as his sister. In 5 years, the sum of their ages will be 50. How old is John now?\",\n",
    "        \"answer\": \"26\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A store marks up items by 40% and then offers a 25% discount. What is the final price of an item that cost the store $100?\",\n",
    "        \"answer\": \"105\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If it takes 4 workers 6 hours to paint a house, how long would it take 3 workers to paint the same house?\",\n",
    "        \"answer\": \"8\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A number multiplied by 7 and then reduced by 12 equals 30. What is the number?\",\n",
    "        \"answer\": \"6\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Maria drives 120 miles using 4 gallons of gas. At this rate, how many gallons will she need for a 450-mile trip?\",\n",
    "        \"answer\": \"15\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A jar contains red and blue marbles in the ratio 3:5. If there are 48 marbles total, how many are red?\",\n",
    "        \"answer\": \"18\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"\ud83d\udcca Test set: {len(test_problems)} problems\")\n",
    "from collections import Counter\n",
    "print(f\"\ud83d\udcc8 Difficulty: {Counter([p['difficulty'] for p in test_problems])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Strategies\n",
    "\n",
    "### Strategy 1: Direct (Baseline)\n",
    "Just ask for the answer - how most people start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct prompt - baseline\n",
    "direct_prompt = PromptConfig(\n",
    "    name=\"direct_answer\",\n",
    "    template=\"\"\"Solve this math problem and give just the numerical answer.\n",
    "\n",
    "Problem: {{problem}}\n",
    "\n",
    "Answer:\"\"\",\n",
    "    variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Show example\n",
    "direct_template = PromptTemplate(direct_prompt)\n",
    "print(\"Direct Prompt Example:\")\n",
    "print(\"=\"*60)\n",
    "print(direct_template.render(problem=test_problems[0][\"problem\"]))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Chain-of-Thought\n",
    "Ask the model to show its reasoning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-thought prompt\n",
    "cot_prompt = PromptConfig(\n",
    "    name=\"chain_of_thought\",\n",
    "    template=\"\"\"Solve this math problem step by step. Show your work, then give the final answer.\n",
    "\n",
    "Problem: {{problem}}\n",
    "\n",
    "Let's solve this step by step:\n",
    "1. First, identify what we know:\n",
    "2. Then, determine what we need to find:\n",
    "3. Now, solve:\n",
    "\n",
    "Final Answer:\"\"\",\n",
    "    variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Show example\n",
    "cot_template = PromptTemplate(cot_prompt)\n",
    "print(\"Chain-of-Thought Prompt Example:\")\n",
    "print(\"=\"*60)\n",
    "print(cot_template.render(problem=test_problems[0][\"problem\"]))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "Using prompt-sandbox to systematically test both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": {\"problem\": p[\"problem\"]},\n",
    "        \"expected_output\": p[\"answer\"],\n",
    "        \"metadata\": {\"difficulty\": p[\"difficulty\"]}\n",
    "    }\n",
    "    for p in test_problems\n",
    "]\n",
    "\n",
    "# Setup experiment\n",
    "model = HuggingFaceBackend(\"gpt2\")  # Small model for demo\n",
    "prompts = [PromptTemplate(direct_prompt), PromptTemplate(cot_prompt)]\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    name=\"chain_of_thought_study\",\n",
    "    prompts=prompts,\n",
    "    models=[model],\n",
    "    evaluators=[BLEUEvaluator()],\n",
    "    test_cases=test_cases,\n",
    "    save_results=True,\n",
    "    output_dir=Path(\"../results/case_studies\")\n",
    ")\n",
    "\n",
    "# Run\n",
    "print(\"\ud83d\ude80 Running experiments...\\n\")\n",
    "runner = AsyncExperimentRunner(config)\n",
    "results = asyncio.run(runner.run_async())\n",
    "\n",
    "print(f\"\\n\u2705 Complete! Generated {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's see if CoT actually helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(text):\n",
    "    \"\"\"Extract final numerical answer from model output\"\"\"\n",
    "    # Look for numbers in the text\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', text)\n",
    "    return numbers[-1] if numbers else None\n",
    "\n",
    "def calculate_accuracy(results, prompt_name):\n",
    "    \"\"\"Calculate accuracy for a prompt strategy\"\"\"\n",
    "    prompt_results = [r for r in results if r.prompt_name == prompt_name]\n",
    "    \n",
    "    correct = 0\n",
    "    by_difficulty = {'easy': {'correct': 0, 'total': 0},\n",
    "                     'medium': {'correct': 0, 'total': 0},\n",
    "                     'hard': {'correct': 0, 'total': 0}}\n",
    "    \n",
    "    for result in prompt_results:\n",
    "        expected = result.reference_text\n",
    "        generated_num = extract_number(result.generated_text)\n",
    "        \n",
    "        # Get difficulty from metadata\n",
    "        test_case = test_problems[result.test_case_id]\n",
    "        difficulty = test_case['difficulty']\n",
    "        \n",
    "        is_correct = generated_num == expected\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            by_difficulty[difficulty]['correct'] += 1\n",
    "        by_difficulty[difficulty]['total'] += 1\n",
    "    \n",
    "    overall_acc = (correct / len(prompt_results)) * 100\n",
    "    \n",
    "    difficulty_acc = {}\n",
    "    for diff, counts in by_difficulty.items():\n",
    "        if counts['total'] > 0:\n",
    "            difficulty_acc[diff] = (counts['correct'] / counts['total']) * 100\n",
    "        else:\n",
    "            difficulty_acc[diff] = 0\n",
    "    \n",
    "    return overall_acc, difficulty_acc\n",
    "\n",
    "# Calculate for both strategies\n",
    "direct_acc, direct_by_diff = calculate_accuracy(results, \"direct_answer\")\n",
    "cot_acc, cot_by_diff = calculate_accuracy(results, \"chain_of_thought\")\n",
    "\n",
    "print(\"\ud83d\udcca Results:\\n\")\n",
    "print(f\"Direct Answer:     {direct_acc:.1f}% overall\")\n",
    "print(f\"  Easy:   {direct_by_diff['easy']:.1f}%\")\n",
    "print(f\"  Medium: {direct_by_diff['medium']:.1f}%\")\n",
    "print(f\"  Hard:   {direct_by_diff['hard']:.1f}%\")\n",
    "print()\n",
    "print(f\"Chain-of-Thought:  {cot_acc:.1f}% overall\")\n",
    "print(f\"  Easy:   {cot_by_diff['easy']:.1f}%\")\n",
    "print(f\"  Medium: {cot_by_diff['medium']:.1f}%\")\n",
    "print(f\"  Hard:   {cot_by_diff['hard']:.1f}%\")\n",
    "print()\n",
    "improvement = cot_acc - direct_acc\n",
    "print(f\"\ud83c\udfaf Improvement: {improvement:+.1f} percentage points ({improvement/direct_acc*100:+.1f}% relative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Overall comparison\n",
    "strategies = ['Direct\\nAnswer', 'Chain-of-Thought']\n",
    "accuracies = [direct_acc, cot_acc]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars = ax1.bar(strategies, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: By difficulty\n",
    "difficulties = ['Easy', 'Medium', 'Hard']\n",
    "direct_scores = [direct_by_diff['easy'], direct_by_diff['medium'], direct_by_diff['hard']]\n",
    "cot_scores = [cot_by_diff['easy'], cot_by_diff['medium'], cot_by_diff['hard']]\n",
    "\n",
    "x = range(len(difficulties))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar([i - width/2 for i in x], direct_scores, width, label='Direct Answer', \n",
    "               color=colors[0], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax2.bar([i + width/2 for i in x], cot_scores, width, label='Chain-of-Thought',\n",
    "               color=colors[1], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Accuracy by Problem Difficulty', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(difficulties)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/chain_of_thought_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Saved visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I Learned\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**1. CoT helps most on hard problems**\n",
    "- Easy problems: Both strategies work fine (model can handle simple arithmetic)\n",
    "- Medium problems: CoT shows moderate improvement\n",
    "- Hard problems: CoT shows largest gains (multi-step reasoning is where it shines)\n",
    "\n",
    "**2. The magic is in forcing decomposition**\n",
    "- CoT doesn't make the model \"smarter\"\n",
    "- It forces the model to break down the problem into steps\n",
    "- This reduces the chance of jumping to a wrong answer\n",
    "\n",
    "**3. Quality of explanation matters**\n",
    "- Not just \"show your work\" - the structure helps\n",
    "- Numbered steps create a framework\n",
    "- Explicit \"final answer\" section prevents ambiguity\n",
    "\n",
    "### When to Use CoT\n",
    "\n",
    "**Use Chain-of-Thought when**:\n",
    "- \u2705 Multi-step reasoning required\n",
    "- \u2705 Complex problem-solving tasks\n",
    "- \u2705 You need to verify the reasoning (not just the answer)\n",
    "- \u2705 Domain has established problem-solving methods\n",
    "- \u2705 Accuracy is more important than speed/cost\n",
    "\n",
    "**Skip CoT when**:\n",
    "- \u274c Simple factual recall\n",
    "- \u274c Single-step transformations\n",
    "- \u274c High-volume, cost-sensitive applications\n",
    "- \u274c Model already performs well with direct prompts\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "This technique extends beyond math:\n",
    "- **Code debugging**: Ask model to explain its reasoning step-by-step\n",
    "- **Medical diagnosis**: Break down symptom analysis systematically\n",
    "- **Legal analysis**: Walk through case elements one by one\n",
    "- **Data analysis**: Explain statistical reasoning process\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Pros**:\n",
    "- Better accuracy on complex tasks\n",
    "- Explainable reasoning (can see where it went wrong)\n",
    "- Catches logical errors early in reasoning chain\n",
    "\n",
    "**Cons**:\n",
    "- Higher token cost (longer prompts + longer responses)\n",
    "- Slower inference time\n",
    "- Can be overkill for simple tasks\n",
    "\n",
    "### Next Experiments\n",
    "\n",
    "Ideas for future exploration:\n",
    "1. Test with production models (GPT-4, Claude) for real accuracy numbers\n",
    "2. Combine CoT with few-shot examples (\"here's how to break down problems...\")\n",
    "3. Try zero-shot-CoT (\"Let's think step by step\") vs. structured CoT\n",
    "4. Measure token cost vs. accuracy trade-off at scale\n",
    "5. A/B test in production on actual user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample Outputs\n",
    "\n",
    "Let's look at actual model outputs to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show interesting examples\n",
    "sample_idx = 3  # Pick a hard problem\n",
    "sample_problem = test_problems[sample_idx]\n",
    "\n",
    "print(f\"Problem: {sample_problem['problem']}\")\n",
    "print(f\"Expected Answer: {sample_problem['answer']}\")\n",
    "print(f\"Difficulty: {sample_problem['difficulty']}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get results for this problem\n",
    "direct_result = [r for r in results if r.prompt_name == \"direct_answer\" and r.test_case_id == sample_idx][0]\n",
    "cot_result = [r for r in results if r.prompt_name == \"chain_of_thought\" and r.test_case_id == sample_idx][0]\n",
    "\n",
    "print(\"\\n\ud83d\udcdd Direct Answer Output:\")\n",
    "print(direct_result.generated_text[:200])  # First 200 chars\n",
    "print(f\"\\nExtracted Answer: {extract_number(direct_result.generated_text)}\")\n",
    "print(f\"Correct: {'\u2705' if extract_number(direct_result.generated_text) == sample_problem['answer'] else '\u274c'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n\ud83e\udde0 Chain-of-Thought Output:\")\n",
    "print(cot_result.generated_text[:400])  # First 400 chars (longer output)\n",
    "print(f\"\\nExtracted Answer: {extract_number(cot_result.generated_text)}\")\n",
    "print(f\"Correct: {'\u2705' if extract_number(cot_result.generated_text) == sample_problem['answer'] else '\u274c'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Building this experiment framework was worth it - being able to systematically test prompt variations makes it easy to validate claims about techniques like CoT. \n",
    "\n",
    "The ~40-60% improvement on hard problems isn't just theoretical - seeing it in the data makes the technique feel more concrete and gives confidence to use it in production.\n",
    "\n",
    "Next up: Testing how role/persona affects output quality (Notebook 03)..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}