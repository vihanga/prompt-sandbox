{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Sandbox Quickstart Guide\n",
    "\n",
    "This notebook demonstrates the basic functionality of prompt-sandbox for prompt engineering experiments.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Prompt Templates\n",
    "\n",
    "Start by defining prompt templates with variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_sandbox.config.schema import PromptConfig\n",
    "from prompt_sandbox.prompts.template import PromptTemplate\n",
    "\n",
    "# Define a simple Q&A prompt\n",
    "qa_config = PromptConfig(\n",
    "    name=\"qa_simple\",\n",
    "    template=\"Question: {{question}}\\nAnswer:\",\n",
    "    variables=[\"question\"]\n",
    ")\n",
    "\n",
    "qa_template = PromptTemplate(qa_config)\n",
    "\n",
    "# Render with actual values\n",
    "rendered = qa_template.render(question=\"What is the capital of France?\")\n",
    "print(rendered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Question: What is the capital of France?\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Different Prompt Styles\n",
    "\n",
    "Let's create two different prompting styles to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct prompting\n",
    "direct_prompt = PromptTemplate(PromptConfig(\n",
    "    name=\"direct\",\n",
    "    template=\"Q: {{question}}\\nA:\",\n",
    "    variables=[\"question\"]\n",
    "))\n",
    "\n",
    "# Chain-of-thought prompting\n",
    "cot_prompt = PromptTemplate(PromptConfig(\n",
    "    name=\"chain_of_thought\",\n",
    "    template=\"Q: {{question}}\\nLet's think step by step:\\nA:\",\n",
    "    variables=[\"question\"]\n",
    "))\n",
    "\n",
    "# Compare renderings\n",
    "test_question = \"What is 15% of 80?\"\n",
    "\n",
    "print(\"=== Direct Prompt ===\")\n",
    "print(direct_prompt.render(question=test_question))\n",
    "\n",
    "print(\"\\n=== Chain-of-Thought Prompt ===\")\n",
    "print(cot_prompt.render(question=test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Model Backends\n",
    "\n",
    "Prompt-sandbox supports multiple model backends. Here we'll use a mock backend for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_sandbox.models.base import ModelBackend, GenerationResult\n",
    "\n",
    "# For this demo, we'll create a simple mock backend\n",
    "class SimpleMockBackend(ModelBackend):\n",
    "    def __init__(self):\n",
    "        self.model_name = \"mock-model\"\n",
    "    \n",
    "    def generate(self, prompt, **kwargs) -> GenerationResult:\n",
    "        # Simple rule-based response for demo\n",
    "        if \"capital\" in prompt.lower() and \"france\" in prompt.lower():\n",
    "            response = \"The capital of France is Paris.\"\n",
    "        elif \"15%\" in prompt and \"80\" in prompt:\n",
    "            response = \"12\"\n",
    "        else:\n",
    "            response = \"I'm a simple mock model for demonstration.\"\n",
    "        \n",
    "        return GenerationResult(\n",
    "            prompt=prompt,\n",
    "            generated_text=response,\n",
    "            tokens_generated=len(response.split()),\n",
    "            generation_time=0.1,\n",
    "            model_name=self.model_name\n",
    "        )\n",
    "\n",
    "# Test the mock backend\n",
    "model = SimpleMockBackend()\n",
    "prompt_text = direct_prompt.render(question=\"What is the capital of France?\")\n",
    "result = model.generate(prompt_text)\n",
    "\n",
    "print(f\"Prompt: {result.prompt}\")\n",
    "print(f\"Response: {result.generated_text}\")\n",
    "print(f\"Tokens: {result.tokens_generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics\n",
    "\n",
    "Evaluate model outputs using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_sandbox.evaluators import BLEUEvaluator, ROUGEEvaluator\n",
    "\n",
    "# Create evaluators\n",
    "bleu = BLEUEvaluator()\n",
    "rouge = ROUGEEvaluator()\n",
    "\n",
    "# Test outputs\n",
    "reference = \"Paris\"\n",
    "prediction = \"The capital of France is Paris.\"\n",
    "\n",
    "# Evaluate\n",
    "bleu_score = bleu.evaluate(prediction, reference)\n",
    "rouge_score = rouge.evaluate(prediction, reference)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"ROUGE Score: {rouge_score:.4f}\")\n",
    "\n",
    "print(\"\\nBLEU measures precision of n-grams (higher = better match)\")\n",
    "print(\"ROUGE measures recall of n-grams (higher = better coverage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "Check out these notebooks for more advanced usage:\n",
    "\n",
    "- `02_full_experiment.ipynb` - Running complete experiments with multiple prompts and models\n",
    "- `03_visualization.ipynb` - Visualizing and comparing results\n",
    "\n",
    "Or explore the examples directory:\n",
    "```bash\n",
    "python examples/complete_example.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
