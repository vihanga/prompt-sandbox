{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Analysis\n",
    "\n",
    "This notebook demonstrates how to visualize experiment results with charts and heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from prompt_sandbox.visualization import ResultVisualizer\n",
    "from prompt_sandbox.experiments.storage import ResultStorage\n",
    "from prompt_sandbox.experiments.comparator import ResultComparator\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results\n",
    "\n",
    "First, load results from a previous experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results (you'll need to run 02_full_experiment.ipynb first)\n",
    "results_dir = Path(\"experiment_results\")\n",
    "\n",
    "if results_dir.exists():\n",
    "    storage = ResultStorage(results_dir)\n",
    "    results = storage.load_results(\"prompt_comparison_demo\")\n",
    "    print(f\"‚úÖ Loaded {len(results)} results\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results found. Run 02_full_experiment.ipynb first.\")\n",
    "    # Create mock results for demo\n",
    "    results = [\n",
    "        {\n",
    "            \"prompt_name\": \"direct\",\n",
    "            \"model_name\": \"model-a\",\n",
    "            \"evaluation_scores\": {\"bleu\": 0.75, \"rouge\": 0.82}\n",
    "        },\n",
    "        {\n",
    "            \"prompt_name\": \"chain_of_thought\",\n",
    "            \"model_name\": \"model-a\",\n",
    "            \"evaluation_scores\": {\"bleu\": 0.68, \"rouge\": 0.79}\n",
    "        },\n",
    "        {\n",
    "            \"prompt_name\": \"direct\",\n",
    "            \"model_name\": \"model-b\",\n",
    "            \"evaluation_scores\": {\"bleu\": 0.72, \"rouge\": 0.85}\n",
    "        },\n",
    "        {\n",
    "            \"prompt_name\": \"chain_of_thought\",\n",
    "            \"model_name\": \"model-b\",\n",
    "            \"evaluation_scores\": {\"bleu\": 0.70, \"rouge\": 0.88}\n",
    "        }\n",
    "    ]\n",
    "    print(\"Using mock results for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Visualizer\n",
    "\n",
    "Initialize the visualization engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResultVisualizer(results)\n",
    "print(\"‚úÖ Visualizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Compare Prompts\n",
    "\n",
    "Bar chart comparing different prompts for a single model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prompts on model-a using BLEU score\n",
    "visualizer.plot_prompt_comparison(\n",
    "    model_name=\"model-a\",\n",
    "    metric=\"bleu\",\n",
    "    output_path=None  # None = display inline\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same comparison using ROUGE score\n",
    "visualizer.plot_prompt_comparison(\n",
    "    model_name=\"model-a\",\n",
    "    metric=\"rouge\",\n",
    "    output_path=None\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Compare Models\n",
    "\n",
    "Bar chart comparing different models for a single prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models using the direct prompt\n",
    "visualizer.plot_model_comparison(\n",
    "    prompt_name=\"direct\",\n",
    "    metric=\"bleu\",\n",
    "    output_path=None\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3: Heatmap Overview\n",
    "\n",
    "Heatmap showing all prompt √ó model combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap for BLEU scores\n",
    "visualizer.plot_metric_heatmap(\n",
    "    metric=\"bleu\",\n",
    "    output_path=None\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for ROUGE scores\n",
    "visualizer.plot_metric_heatmap(\n",
    "    metric=\"rouge\",\n",
    "    output_path=None\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "Get detailed statistics using the comparator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator = ResultComparator(results)\n",
    "\n",
    "# Compare all prompts for model-a\n",
    "prompt_comparison = comparator.compare_prompts(\"model-a\", \"bleu\")\n",
    "\n",
    "print(\"üìä Prompt Comparison (model-a, BLEU):\")\n",
    "print(\"-\" * 50)\n",
    "for prompt_name, stats in prompt_comparison.items():\n",
    "    print(f\"{prompt_name:20s}: {stats['mean']:.3f} ¬± {stats['std']:.3f}\")\n",
    "    print(f\"{'':20s}  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models for direct prompt\n",
    "model_comparison = comparator.compare_models(\"direct\", \"bleu\")\n",
    "\n",
    "print(\"üìä Model Comparison (direct prompt, BLEU):\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, stats in model_comparison.items():\n",
    "    print(f\"{model_name:20s}: {stats['mean']:.3f} ¬± {stats['std']:.3f}\")\n",
    "    print(f\"{'':20s}  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Winners\n",
    "\n",
    "Identify best configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÜ Best Configurations:\\n\")\n",
    "\n",
    "# Best prompt for each model\n",
    "for model in [\"model-a\", \"model-b\"]:\n",
    "    for metric in [\"bleu\", \"rouge\"]:\n",
    "        best_prompt, score = comparator.get_best_prompt(model, metric)\n",
    "        print(f\"Best for {model} ({metric.upper()}): {best_prompt} (score: {score:.3f})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Best model for each prompt\n",
    "for prompt in [\"direct\", \"chain_of_thought\"]:\n",
    "    for metric in [\"bleu\", \"rouge\"]:\n",
    "        best_model, score = comparator.get_best_model(prompt, metric)\n",
    "        print(f\"Best for {prompt} ({metric.upper()}): {best_model} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Plots to Files\n",
    "\n",
    "Export visualizations for reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for plots\n",
    "plots_dir = Path(\"plots\")\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save prompt comparison\n",
    "visualizer.plot_prompt_comparison(\n",
    "    model_name=\"model-a\",\n",
    "    metric=\"bleu\",\n",
    "    output_path=plots_dir / \"prompt_comparison.png\"\n",
    ")\n",
    "\n",
    "# Save model comparison\n",
    "visualizer.plot_model_comparison(\n",
    "    prompt_name=\"direct\",\n",
    "    metric=\"bleu\",\n",
    "    output_path=plots_dir / \"model_comparison.png\"\n",
    ")\n",
    "\n",
    "# Save heatmaps\n",
    "visualizer.plot_metric_heatmap(\n",
    "    metric=\"bleu\",\n",
    "    output_path=plots_dir / \"heatmap_bleu.png\"\n",
    ")\n",
    "\n",
    "visualizer.plot_metric_heatmap(\n",
    "    metric=\"rouge\",\n",
    "    output_path=plots_dir / \"heatmap_rouge.png\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Plots saved to: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Analysis\n",
    "\n",
    "Create your own visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract data for custom plotting\n",
    "prompts = sorted(set(r[\"prompt_name\"] for r in results))\n",
    "models = sorted(set(r[\"model_name\"] for r in results))\n",
    "\n",
    "# Create scatter plot of BLEU vs ROUGE\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for prompt in prompts:\n",
    "    bleu_scores = [r[\"evaluation_scores\"].get(\"bleu\", 0) \n",
    "                   for r in results if r[\"prompt_name\"] == prompt]\n",
    "    rouge_scores = [r[\"evaluation_scores\"].get(\"rouge\", 0) \n",
    "                    for r in results if r[\"prompt_name\"] == prompt]\n",
    "    \n",
    "    ax.scatter(bleu_scores, rouge_scores, label=prompt, s=100, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('BLEU Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('ROUGE Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('BLEU vs ROUGE Correlation', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Loading experiment results\n",
    "- Creating bar charts for prompt and model comparisons\n",
    "- Generating heatmaps for overview analysis\n",
    "- Statistical comparison with mean and standard deviation\n",
    "- Finding best configurations\n",
    "- Saving plots to files\n",
    "- Custom visualization with matplotlib\n",
    "\n",
    "All visualizations are publication-ready and can be included in reports!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
