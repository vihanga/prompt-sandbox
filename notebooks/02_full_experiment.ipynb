{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Experiment Workflow\n",
    "\n",
    "This notebook demonstrates how to run complete experiments comparing multiple prompts across models.\n",
    "\n",
    "**Note**: This notebook uses async execution for 10-50x speedup over sequential processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from prompt_sandbox.config.schema import PromptConfig\n",
    "from prompt_sandbox.prompts.template import PromptTemplate\n",
    "from prompt_sandbox.evaluators import BLEUEvaluator, ROUGEEvaluator, BERTScoreEvaluator\n",
    "from prompt_sandbox.experiments import AsyncExperimentRunner, ExperimentConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create Mock Model for Demo\n",
    "\n",
    "For this demo, we'll use a mock model. In real usage, you'd use `OllamaBackend` or `HuggingFaceBackend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_sandbox.models.base import ModelBackend, GenerationResult\n",
    "\n",
    "class DemoModel(ModelBackend):\n",
    "    \"\"\"Mock model that simulates different response patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, response_style: str = \"direct\"):\n",
    "        self.model_name = name\n",
    "        self.response_style = response_style\n",
    "    \n",
    "    async def generate_async(self, prompt: str, **kwargs) -> GenerationResult:\n",
    "        await asyncio.sleep(0.1)  # Simulate API latency\n",
    "        \n",
    "        # Simple pattern matching for demo responses\n",
    "        if \"2+2\" in prompt or \"2 + 2\" in prompt:\n",
    "            if self.response_style == \"verbose\":\n",
    "                response = \"Let me calculate that. 2 plus 2 equals 4.\"\n",
    "            else:\n",
    "                response = \"4\"\n",
    "        elif \"capital\" in prompt.lower() and \"france\" in prompt.lower():\n",
    "            if self.response_style == \"verbose\":\n",
    "                response = \"The capital city of France is Paris.\"\n",
    "            else:\n",
    "                response = \"Paris\"\n",
    "        elif \"python\" in prompt.lower() and \"invented\" in prompt.lower():\n",
    "            if self.response_style == \"verbose\":\n",
    "                response = \"Python was invented by Guido van Rossum.\"\n",
    "            else:\n",
    "                response = \"Guido van Rossum\"\n",
    "        else:\n",
    "            response = \"I don't know.\"\n",
    "        \n",
    "        return GenerationResult(\n",
    "            prompt=prompt,\n",
    "            generated_text=response,\n",
    "            tokens_generated=len(response.split()),\n",
    "            generation_time=0.1,\n",
    "            model_name=self.model_name\n",
    "        )\n",
    "    \n",
    "    def generate(self, prompt: str, **kwargs) -> GenerationResult:\n",
    "        return asyncio.run(self.generate_async(prompt, **kwargs))\n",
    "\n",
    "# Create two different models\n",
    "model_a = DemoModel(\"model-a\", response_style=\"direct\")\n",
    "model_b = DemoModel(\"model-b\", response_style=\"verbose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompts to Test\n",
    "\n",
    "Let's create three different prompting strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Direct question\n",
    "prompt_direct = PromptTemplate(PromptConfig(\n",
    "    name=\"direct\",\n",
    "    template=\"Q: {{question}}\\nA:\",\n",
    "    variables=[\"question\"]\n",
    "))\n",
    "\n",
    "# Strategy 2: Chain-of-thought\n",
    "prompt_cot = PromptTemplate(PromptConfig(\n",
    "    name=\"chain_of_thought\",\n",
    "    template=\"Q: {{question}}\\nLet's think step by step and answer:\\nA:\",\n",
    "    variables=[\"question\"]\n",
    "))\n",
    "\n",
    "# Strategy 3: Instructional\n",
    "prompt_instructional = PromptTemplate(PromptConfig(\n",
    "    name=\"instructional\",\n",
    "    template=\"Answer the following question concisely.\\nQuestion: {{question}}\\nAnswer:\",\n",
    "    variables=[\"question\"]\n",
    "))\n",
    "\n",
    "prompts = [prompt_direct, prompt_cot, prompt_instructional]\n",
    "print(f\"Created {len(prompts)} prompt templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Cases\n",
    "\n",
    "Define questions with expected answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is 2+2?\"},\n",
    "        \"expected_output\": \"4\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is the capital of France?\"},\n",
    "        \"expected_output\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"Who invented Python?\"},\n",
    "        \"expected_output\": \"Guido van Rossum\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Experiment\n",
    "\n",
    "Set up the experiment with prompts, models, and evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"experiment_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure experiment\n",
    "config = ExperimentConfig(\n",
    "    name=\"prompt_comparison_demo\",\n",
    "    prompts=prompts,\n",
    "    models=[model_a, model_b],\n",
    "    evaluators=[\n",
    "        BLEUEvaluator(),\n",
    "        ROUGEEvaluator(),\n",
    "        # BERTScoreEvaluator()  # Commented out for faster demo\n",
    "    ],\n",
    "    test_cases=test_cases,\n",
    "    save_results=True,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"Experiment configuration:\")\n",
    "print(f\"  - {len(config.prompts)} prompts\")\n",
    "print(f\"  - {len(config.models)} models\")\n",
    "print(f\"  - {len(config.test_cases)} test cases\")\n",
    "print(f\"  - Total runs: {len(config.prompts) * len(config.models) * len(config.test_cases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment (Async)\n",
    "\n",
    "Execute all combinations in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "runner = AsyncExperimentRunner(config)\n",
    "results = await runner.run_async()\n",
    "\n",
    "print(f\"\\n‚úÖ Experiment complete! Generated {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Get summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = runner.get_summary()\n",
    "\n",
    "print(\"\\nüìä Summary Statistics:\\n\")\n",
    "for (prompt_name, model_name), stats in summary.items():\n",
    "    print(f\"=== {prompt_name} + {model_name} ===\")\n",
    "    for metric, values in stats[\"scores\"].items():\n",
    "        print(f\"  {metric.upper()}: {values['mean']:.3f} (¬±{values['std']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Configuration\n",
    "\n",
    "Use the comparator to identify winners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_sandbox.experiments.comparator import ResultComparator\n",
    "\n",
    "comparator = ResultComparator(results)\n",
    "\n",
    "# Find best prompt for each model\n",
    "for model_name in [\"model-a\", \"model-b\"]:\n",
    "    print(f\"\\nüèÜ Best prompts for {model_name}:\")\n",
    "    for metric in [\"bleu\", \"rouge\"]:\n",
    "        best_prompt, score = comparator.get_best_prompt(model_name, metric)\n",
    "        print(f\"  {metric.upper()}: {best_prompt} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Individual Results\n",
    "\n",
    "Look at actual prompts and responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few results in detail\n",
    "print(\"\\nüîç Sample Results:\\n\")\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Prompt: {result['prompt_name']}\")\n",
    "    print(f\"  Model: {result['model_name']}\")\n",
    "    print(f\"  Input: {result['input']}\")\n",
    "    print(f\"  Expected: {result['expected_output']}\")\n",
    "    print(f\"  Actual: {result['actual_output']}\")\n",
    "    print(f\"  Scores: BLEU={result['evaluation_scores'].get('bleu', 0):.3f}, \"\n",
    "          f\"ROUGE={result['evaluation_scores'].get('rouge', 0):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Results\n",
    "\n",
    "Results are automatically saved. You can reload them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_sandbox.experiments.storage import ResultStorage\n",
    "\n",
    "# Load saved results\n",
    "storage = ResultStorage(output_dir)\n",
    "loaded_results = storage.load_results(\"prompt_comparison_demo\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(loaded_results)} results from disk\")\n",
    "print(f\"Results saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See `03_visualization.ipynb` for plotting and visualization\n",
    "- Try with real models: `OllamaBackend('llama3.1')` or `HuggingFaceBackend('meta-llama/Llama-2-7b')`\n",
    "- Add more evaluators like `BERTScoreEvaluator()` for semantic similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
