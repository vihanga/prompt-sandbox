# Prompt-Sandbox Development Documentation

**Last Updated**: January 2025
**Status**: Active Development
**Lead**: Vihanga Gamage

---

## ðŸŽ¯ Project Vision

Build a production-ready framework for systematic prompt engineering that enables:
1. **Rapid experimentation** with prompt variations
2. **Objective evaluation** across multiple LLMs
3. **Reproducible results** through config-driven workflows
4. **Insight discovery** through comparative analysis

---

## ðŸ—ï¸ Architecture Overview

### High-Level Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Prompt-Sandbox                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Config     â”‚â”€â”€â”€â–¶â”‚  Experiment  â”‚â”€â”€â”€â–¶â”‚  Evaluation  â”‚ â”‚
â”‚  â”‚   Manager    â”‚    â”‚   Runner     â”‚    â”‚   Engine     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                    â”‚        â”‚
â”‚         â–¼                    â–¼                    â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    YAML      â”‚    â”‚    Model     â”‚    â”‚   Metrics    â”‚ â”‚
â”‚  â”‚   Prompts    â”‚    â”‚   Loaders    â”‚    â”‚  (BLEU,      â”‚ â”‚
â”‚  â”‚              â”‚    â”‚   (Hugging   â”‚    â”‚   BERTScore, â”‚ â”‚
â”‚  â”‚              â”‚    â”‚    Face)     â”‚    â”‚   Custom)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Results Dashboard & Comparison            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

#### 1. Config Manager
- **Purpose**: Load and validate YAML prompt configurations
- **Key Features**:
  - Schema validation (Pydantic models)
  - Variable interpolation
  - Config inheritance/composition
  - Version tracking

#### 2. Experiment Runner
- **Purpose**: Orchestrate multi-model inference
- **Key Features**:
  - Async/parallel execution
  - Progress tracking
  - Error handling & retry logic
  - Caching for expensive operations

#### 3. Model Loaders
- **Purpose**: Abstract model loading/inference
- **Supported Backends**:
  - Hugging Face Transformers
  - vLLM (optional, for speed)
  - Llama.cpp (optional, for quantized models)
  - OpenAI API (optional, for GPT comparisons)

#### 4. Evaluation Engine
- **Purpose**: Score outputs against ground truth/references
- **Metrics**:
  - **BLEU**: N-gram overlap (translation quality)
  - **BERTScore**: Semantic similarity via BERT embeddings
  - **ROUGE**: Recall-oriented summarization metric
  - **Faithfulness**: Custom NLI-based factual consistency
  - **Perplexity**: Model confidence (internal metric)

---

## ðŸ“‚ Directory Structure (Detailed)

```
prompt-sandbox/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ prompt_sandbox/        # Main package (snake_case)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ schema.py          # Pydantic models for validation
â”‚       â”‚   â”œâ”€â”€ loader.py          # YAML loading with Hydra
â”‚       â”‚   â””â”€â”€ validator.py       # Config consistency checks
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py            # Abstract model interface
â”‚       â”‚   â”œâ”€â”€ hf_model.py        # Hugging Face implementation
â”‚       â”‚   â”œâ”€â”€ vllm_model.py      # vLLM implementation (optional)
â”‚       â”‚   â””â”€â”€ loader.py          # Model factory pattern
â”‚       â”œâ”€â”€ prompts/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ template.py        # Jinja2 template rendering
â”‚       â”‚   â”œâ”€â”€ stack.py           # System+role+content composition
â”‚       â”‚   â””â”€â”€ variables.py       # Variable injection logic
â”‚       â”œâ”€â”€ evaluators/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py            # Abstract evaluator
â”‚       â”‚   â”œâ”€â”€ bleu.py            # BLEU score implementation
â”‚       â”‚   â”œâ”€â”€ bertscore.py       # BERTScore wrapper
â”‚       â”‚   â”œâ”€â”€ rouge.py           # ROUGE metrics
â”‚       â”‚   â”œâ”€â”€ faithfulness.py    # NLI-based checker
â”‚       â”‚   â””â”€â”€ perplexity.py      # Model-intrinsic metric
â”‚       â”œâ”€â”€ experiments/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ runner.py          # Main experiment orchestration
â”‚       â”‚   â”œâ”€â”€ results.py         # Results data models
â”‚       â”‚   â””â”€â”€ comparison.py      # Side-by-side analysis
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ logging.py         # Structured logging setup
â”‚           â”œâ”€â”€ caching.py         # Result caching (joblib)
â”‚           â””â”€â”€ metrics.py         # Metric aggregation helpers
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ qa_assistant.yaml
â”‚   â”‚   â”œâ”€â”€ summarization.yaml
â”‚   â”‚   â”œâ”€â”€ code_generation.yaml
â”‚   â”‚   â””â”€â”€ chain_of_thought.yaml
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ truthfulqa_eval.yaml
â”‚   â”‚   â”œâ”€â”€ gsm8k_eval.yaml
â”‚   â”‚   â””â”€â”€ custom_benchmark.yaml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ llama2_7b.yaml
â”‚       â”œâ”€â”€ mistral_7b.yaml
â”‚       â””â”€â”€ phi2.yaml
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_config.py
â”‚   â”‚   â”œâ”€â”€ test_prompts.py
â”‚   â”‚   â”œâ”€â”€ test_evaluators.py
â”‚   â”‚   â””â”€â”€ test_models.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_experiment.py
â”‚   â”‚   â””â”€â”€ test_end_to_end.py
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ sample_prompts.yaml
â”‚       â””â”€â”€ sample_outputs.json
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ quickstart.py
â”‚   â”œâ”€â”€ custom_evaluator.py
â”‚   â”œâ”€â”€ batch_comparison.py
â”‚   â””â”€â”€ notebooks/
â”‚       â”œâ”€â”€ 01_basic_usage.ipynb
â”‚       â”œâ”€â”€ 02_custom_metrics.ipynb
â”‚       â””â”€â”€ 03_advanced_prompting.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_datasets.py
â”‚   â”œâ”€â”€ run_benchmark.py
â”‚   â””â”€â”€ generate_report.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â”œâ”€â”€ prompt_design_guide.md
â”‚   â””â”€â”€ evaluation_metrics.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ _dev/                      # Public development docs (in git)
    â”œâ”€â”€ architecture.md (this file)
    â”œâ”€â”€ implementation_notes.md
    â”œâ”€â”€ technical_challenges.md
    â””â”€â”€ api_reference.md
```

---

## ðŸ”§ Technical Implementation Details

### 1. YAML Prompt Configuration

**Schema Design** (Pydantic):

```python
# src/prompt_sandbox/config/schema.py
from pydantic import BaseModel, Field
from typing import List, Dict, Optional

class PromptConfig(BaseModel):
    """Schema for prompt configuration files"""

    name: str = Field(..., description="Unique prompt identifier")
    version: str = Field(default="1.0", description="Prompt version")

    system: Optional[str] = Field(None, description="System message")
    role: str = Field(default="assistant", description="Role identifier")

    template: str = Field(..., description="Jinja2 template string")
    variables: List[str] = Field(default=[], description="Required variables")

    metadata: Optional[Dict[str, any]] = Field(
        default={},
        description="Additional metadata (tags, author, etc.)"
    )

    few_shot_examples: Optional[List[Dict[str, str]]] = Field(
        default=None,
        description="Few-shot learning examples"
    )
```

**Example YAML**:

```yaml
# configs/prompts/qa_assistant.yaml
name: "qa_assistant_v1"
version: "1.0"

system: |
  You are a helpful AI assistant that provides accurate, concise answers.
  Always cite sources when possible and admit uncertainty when appropriate.

role: "assistant"

template: |
  Question: {{ question }}

  {% if context %}
  Context: {{ context }}
  {% endif %}

  Please provide a clear, factual answer.

variables:
  - question
  - context  # Optional

metadata:
  tags: ["qa", "general"]
  author: "portfolio"
  created: "2025-01-08"

few_shot_examples:
  - question: "What is the capital of France?"
    answer: "Paris is the capital of France."
  - question: "Who wrote Hamlet?"
    answer: "William Shakespeare wrote Hamlet."
```

### 2. Model Abstraction Layer

**Interface Design**:

```python
# src/prompt_sandbox/models/base.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class GenerationConfig:
    """Configuration for text generation"""
    max_new_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    num_return_sequences: int = 1
    do_sample: bool = True

class BaseModel(ABC):
    """Abstract base class for all LLM backends"""

    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device

    @abstractmethod
    def load(self) -> None:
        """Load model into memory"""
        pass

    @abstractmethod
    def generate(
        self,
        prompt: str,
        config: GenerationConfig
    ) -> List[str]:
        """Generate text from prompt"""
        pass

    @abstractmethod
    def get_perplexity(self, text: str) -> float:
        """Calculate perplexity of text"""
        pass

    def unload(self) -> None:
        """Free model from memory"""
        pass
```

**Hugging Face Implementation**:

```python
# src/prompt_sandbox/models/hf_model.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from .base import BaseModel, GenerationConfig

class HuggingFaceModel(BaseModel):
    """Hugging Face Transformers implementation"""

    def load(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=self.device,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )

    def generate(self, prompt: str, config: GenerationConfig) -> List[str]:
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=config.max_new_tokens,
            temperature=config.temperature,
            top_p=config.top_p,
            top_k=config.top_k,
            num_return_sequences=config.num_return_sequences,
            do_sample=config.do_sample,
            pad_token_id=self.tokenizer.eos_token_id
        )

        return [
            self.tokenizer.decode(out, skip_special_tokens=True)
            for out in outputs
        ]

    def get_perplexity(self, text: str) -> float:
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model(**inputs, labels=inputs["input_ids"])

        return torch.exp(outputs.loss).item()
```

### 3. Evaluation Metrics Implementation

**Faithfulness Checker** (Advanced):

```python
# src/prompt_sandbox/evaluators/faithfulness.py
from transformers import pipeline
from .base import BaseEvaluator

class FaithfulnessEvaluator(BaseEvaluator):
    """
    Uses Natural Language Inference (NLI) to check if generated
    text is entailed by the source context.
    """

    def __init__(self, model_name: str = "facebook/bart-large-mnli"):
        self.nli_pipeline = pipeline(
            "text-classification",
            model=model_name,
            device=0 if torch.cuda.is_available() else -1
        )

    def evaluate(
        self,
        generated_text: str,
        context: str,
        threshold: float = 0.5
    ) -> Dict[str, Any]:
        """
        Compute faithfulness score.

        Returns:
            - score: Float [0, 1], higher = more faithful
            - label: "entailment" | "neutral" | "contradiction"
            - confidence: Model confidence
        """
        # Split generated text into sentences
        sentences = self._split_sentences(generated_text)

        scores = []
        for sent in sentences:
            result = self.nli_pipeline(
                f"{context} [SEP] {sent}",
                return_all_scores=True
            )[0]

            # Find entailment score
            entailment_score = next(
                r["score"] for r in result if r["label"] == "ENTAILMENT"
            )
            scores.append(entailment_score)

        avg_score = sum(scores) / len(scores) if scores else 0.0

        return {
            "score": avg_score,
            "label": "faithful" if avg_score >= threshold else "unfaithful",
            "confidence": avg_score,
            "sentence_scores": scores
        }

    def _split_sentences(self, text: str) -> List[str]:
        """Simple sentence splitting (can be improved with spaCy)"""
        import re
        return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
```

### 4. Experiment Orchestration

**Async Execution Pattern**:

```python
# src/prompt_sandbox/experiments/runner.py
import asyncio
from typing import List, Dict
from dataclasses import dataclass
from ..models import BaseModel
from ..evaluators import BaseEvaluator
from ..prompts import PromptTemplate

@dataclass
class ExperimentConfig:
    """Configuration for an experiment run"""
    prompt_configs: List[str]  # Paths to prompt YAML files
    models: List[str]  # Model identifiers
    eval_dataset: str  # Path to evaluation dataset
    evaluators: List[BaseEvaluator]
    num_samples: int = 100
    batch_size: int = 8

class ExperimentRunner:
    """Orchestrates multi-model, multi-prompt experiments"""

    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.results = []

    async def run(self) -> Dict[str, Any]:
        """Execute experiment with async parallel execution"""

        # Load all components
        prompts = self._load_prompts()
        models = self._load_models()
        dataset = self._load_dataset()

        # Create task matrix: prompts Ã— models Ã— samples
        tasks = []
        for prompt in prompts:
            for model in models:
                for sample in dataset[:self.config.num_samples]:
                    tasks.append(
                        self._run_single(prompt, model, sample)
                    )

        # Execute with concurrency limit
        results = await self._execute_batched(tasks, self.config.batch_size)

        # Aggregate and analyze
        return self._analyze_results(results)

    async def _run_single(
        self,
        prompt: PromptTemplate,
        model: BaseModel,
        sample: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute single inference + evaluation"""

        # Render prompt with sample variables
        rendered_prompt = prompt.render(**sample)

        # Generate
        outputs = model.generate(rendered_prompt)

        # Evaluate
        scores = {}
        for evaluator in self.config.evaluators:
            scores[evaluator.name] = evaluator.evaluate(
                generated=outputs[0],
                reference=sample.get("reference", ""),
                context=sample.get("context", "")
            )

        return {
            "prompt_name": prompt.name,
            "model_name": model.model_name,
            "sample_id": sample["id"],
            "output": outputs[0],
            "scores": scores
        }

    async def _execute_batched(
        self,
        tasks: List,
        batch_size: int
    ) -> List[Dict]:
        """Execute tasks in batches to control memory"""
        results = []

        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i + batch_size]
            batch_results = await asyncio.gather(*batch)
            results.extend(batch_results)

            # Log progress
            print(f"Completed {len(results)}/{len(tasks)} tasks")

        return results
```

---

## ðŸ“Š Evaluation Strategy

### Metrics Selection Guide

| Metric | Best For | Pros | Cons |
|--------|----------|------|------|
| BLEU | Translation, exact match | Fast, established | Ignores semantics |
| BERTScore | Semantic similarity | Context-aware | Slower, requires BERT |
| ROUGE | Summarization | Recall-focused | N-gram based |
| Faithfulness | Factual accuracy | Catches hallucinations | Requires context |
| Perplexity | Model confidence | Intrinsic | Not human-aligned |

### Benchmark Datasets

1. **TruthfulQA** (817 questions)
   - Tests model truthfulness
   - Categories: health, law, science, politics
   - Metric: % truthful + informative answers

2. **GSM8K** (8.5K grade school math problems)
   - Tests reasoning ability
   - Metric: Exact match accuracy

3. **MMLU** (Massive Multitask Language Understanding)
   - 57 subjects across STEM, humanities, social sciences
   - Metric: Multiple-choice accuracy

---

## ðŸš€ Development Phases

### Phase 1: Core Infrastructure (Week 1)
- [x] Directory structure
- [ ] Pydantic schemas for configs
- [ ] YAML loader with Hydra
- [ ] Base model interface
- [ ] HuggingFace model implementation
- [ ] Basic prompt rendering (Jinja2)

### Phase 2: Evaluation Suite (Week 2)
- [ ] BLEU evaluator
- [ ] BERTScore evaluator
- [ ] Faithfulness evaluator
- [ ] Aggregated metrics reporting
- [ ] Unit tests for evaluators

### Phase 3: Experiment System (Week 3)
- [ ] Experiment runner with async
- [ ] Results storage (JSON/SQLite)
- [ ] Progress tracking & logging
- [ ] Caching mechanism
- [ ] Integration tests

### Phase 4: Comparison & Visualization (Week 4)
- [ ] Side-by-side comparison tool
- [ ] Statistical significance tests
- [ ] Visualization dashboard (Plotly)
- [ ] Report generation (PDF/HTML)

---

## ðŸ§ª Testing Strategy

### Unit Tests
- Config validation edge cases
- Prompt rendering with various inputs
- Metric calculations with known examples
- Model interface mocking

### Integration Tests
- End-to-end experiment runs (small dataset)
- Multi-model comparisons
- Error handling & recovery

### Performance Tests
- Async execution speedup verification
- Memory usage monitoring
- Cache hit rate analysis

---

## ðŸ“ˆ Success Metrics

| Metric | Target | Current |
|--------|--------|---------|
| Unit Test Coverage | >85% | 0% |
| Integration Tests | >5 scenarios | 0 |
| Experiment Runtime (100 samples) | <5 min | N/A |
| Memory Usage (3 models loaded) | <16GB | N/A |
| Config Validation Errors | 100% caught | N/A |

---

## ðŸ”® Future Enhancements

### Short-term (Next 2-4 weeks)
- [ ] vLLM backend for 10x faster inference
- [ ] OpenAI API integration for GPT comparisons
- [ ] Chain-of-thought prompt templates
- [ ] Few-shot learning examples in configs

### Medium-term (1-2 months)
- [ ] Web UI for experiment management (Streamlit)
- [ ] Automatic hyperparameter tuning (Optuna)
- [ ] Multi-turn conversation evaluation
- [ ] Cost estimation for API-based models

### Long-term (3+ months)
- [ ] LoRA fine-tuning integration
- [ ] Custom reward modeling
- [ ] Active learning for prompt discovery
- [ ] Multi-language support

---

## ðŸ“š References & Resources

### Key Papers
1. **"Scaling Instruction-Finetuned Language Models"** (Chung et al., 2022)
   - Flan-T5 prompt design patterns

2. **"Chain-of-Thought Prompting"** (Wei et al., 2022)
   - Advanced prompting techniques

3. **"BERTScore: Evaluating Text Generation with BERT"** (Zhang et al., 2019)
   - Semantic evaluation metrics

### Useful Tools
- **Hugging Face Transformers**: Model library
- **Hydra**: Configuration management
- **Pydantic**: Data validation
- **pytest**: Testing framework

---

## ðŸ‘¥ Development Notes

### Code Style
- **Formatter**: Black (line length: 88)
- **Linter**: Flake8 + mypy (type checking)
- **Docstrings**: Google style
- **Imports**: isort for organization

### Git Workflow
- **Main branch**: `main` (protected)
- **Feature branches**: `feature/prompt-rendering`, `feature/bert-evaluator`
- **Commit messages**: Conventional commits (`feat:`, `fix:`, `docs:`)

### Performance Considerations
- Use `torch.float16` for inference (2x memory reduction)
- Enable `gradient_checkpointing` for large models
- Cache tokenized inputs when possible
- Use `accelerate` for multi-GPU support

---

**Next Steps**: Implement Phase 1 core infrastructure (estimated: 3-5 days)
